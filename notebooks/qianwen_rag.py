import os
import dashscope
from dashscope import Generation, TextEmbedding
import chromadb
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("DASHSCOPE_API_KEY")

# --- 1. å‡†å¤‡å·¥ä½œï¼šæ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„çŸ¥è¯†åº“ ---
# åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œè¿™é‡Œæ˜¯ä½ è¯»å– PDF/TXT çš„ä»£ç 
documents = [
    "Qwen3æ˜¯é˜¿é‡Œäº‘åœ¨2025å¹´å‘å¸ƒçš„æœ€æ–°ä¸€ä»£è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚",
    "Qwen3ç›¸æ¯”å‰ä»£ï¼Œåœ¨é€»è¾‘æ¨ç†å’Œä»£ç èƒ½åŠ›ä¸Šæå‡äº†40%ã€‚",
    "ç”¨æˆ·æœ‰ä¸€åªå® ç‰©çŒ«",
    "å¤§è±¡å…¶å®å¾ˆå®³æ€•è€é¼ ï¼Œå› ä¸ºè€é¼ ä¼šé’»è¿›å®ƒä»¬çš„é¼»å­é‡Œï¼ˆè¿™æ˜¯ä¸ªè°£è¨€ï¼‰ã€‚"
]

# åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆè¿™é‡Œç”¨å†…å­˜æ¨¡å¼ï¼Œé‡å¯å°±æ²¡äº†ï¼Œæ–¹ä¾¿æµ‹è¯•ï¼‰
print("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“...")
chroma_client = chromadb.Client()
collection = chroma_client.create_collection(name="my_knowledge_base")

# æŠŠæ–‡æœ¬å˜æˆå‘é‡å­˜è¿›å» (è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ç”¨ dashscope æä¾›çš„ embedding)
# çœŸå® RAG è¿™ä¸€æ­¥é€šå¸¸æ˜¯ç¦»çº¿åšå¥½çš„
def get_embedding(text):
    resp = TextEmbedding.call(
        model=TextEmbedding.Models.text_embedding_v1,
        input=text,
        api_key=api_key
    )
    return resp.output.embeddings[0].embedding

# å­˜å…¥æ•°æ®åº“
for i, doc in enumerate(documents):
    # è¿™é‡Œå·æ‡’äº†ï¼ŒçœŸå®åœºæ™¯å»ºè®®ç”¨ batch æ‰¹é‡å¤„ç†
    emb = get_embedding(doc)
    collection.add(
        ids=[str(i)],
        embeddings=[emb],
        documents=[doc]
    )
print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼(å­˜äº†4æ¡æ•°æ®)")


# --- 2. æ£€ç´¢ + ç”Ÿæˆ ---
def chat_with_rag():
    print("--- RAG åŠ©æ‰‹ (é—®é—®æˆ‘å…³äº Qwen3 çš„äº‹) ---")
    while True:
        query = input("\nä½ : ")
        if query in ['quit', 'exit']: break
        
        # [æ­¥éª¤A] æ£€ç´¢ï¼šå…ˆå»åº“é‡Œæ‰¾ç­”æ¡ˆ
        print("ğŸ” æ­£åœ¨æ£€ç´¢èµ„æ–™...", end="")
        query_emb = get_embedding(query)
        results = collection.query(
            query_embeddings=[query_emb],
            n_results=2 # åªæ‰¾æœ€ç›¸å…³çš„2æ¡
        )
        retrieved_docs = results['documents'][0]
        print(f"æ‰¾åˆ° {len(retrieved_docs)} æ¡ç›¸å…³èµ„æ–™")
        
        # [æ­¥éª¤B] å¢å¼ºï¼šæŠŠèµ„æ–™å¡è¿› Prompt
        # è¿™å°±æ˜¯ RAG çš„çµé­‚ï¼šè®©æ¨¡å‹â€œçœ‹ç€ç­”æ¡ˆä½œå¼Šâ€
        context_str = "\n".join([f"- {doc}" for doc in retrieved_docs])
        
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†åº“çš„åŠ©æ‰‹ã€‚
        è¯·åŠ¡å¿…åªæ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœèµ„æ–™é‡Œæ²¡æåˆ°ï¼Œå°±è¯´ä¸çŸ¥é“ã€‚
        
        ã€å‚è€ƒèµ„æ–™ã€‘
        {context_str}
        """
        
        # [æ­¥éª¤C] ç”Ÿæˆï¼šå¸¦ç€èµ„æ–™å»é—® LLM
        messages = [
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': query}
        ]
        
        print("AIæ­£åœ¨æ€è€ƒ...")
        resp = Generation.call(
            model='qwen-max',
            api_key=api_key,
            messages=messages,
            result_format='message'
        )
        
        print(f"ğŸ¤– AI: {resp.output.choices[0]['message']['content']}")

if __name__ == "__main__":
    chat_with_rag()
